{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GqQs8iOXQbBP"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCWL7SQxUKRWE49jFqS9OZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AugustinCablant/Viager/blob/main/Bases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code Stage Augustin Cablant (2023)**\n",
        "\n",
        "> Ce document regroupe les lignes de code utilisées pour mon stage."
      ],
      "metadata": {
        "id": "8CLtCVw3QXw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "GqQs8iOXQbBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fn-16JGqQURa",
        "outputId": "cd823ef3-f393-4625-ee66-e124679b4e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.0.4-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=9.0.0 (from reportlab)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow, reportlab\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed pillow-9.5.0 reportlab-4.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfkit\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pdfkit\n",
            "Successfully installed pdfkit-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser) (2022.7.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2022.10.31)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser) (4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser) (1.16.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser) (2023.3)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 rapidfuzz-3.1.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install reportlab\n",
        "!pip install pdfkit\n",
        "!pip install tqdm\n",
        "!pip install dateparser\n",
        "!pip install unidecode\n",
        "!pip install Levenshtein\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import locale\n",
        "import matplotlib.pyplot as plt\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.platypus import SimpleDocTemplate, Table\n",
        "import pdfkit\n",
        "import tabulate\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle\n",
        "from reportlab.lib import colors\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "import html\n",
        "import dateparser\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Stage')\n",
        "from tqdm import tqdm\n",
        "from unidecode import unidecode\n",
        "import Levenshtein\n",
        "from nltk.metrics.distance import jaccard_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1 : création de la base de travail"
      ],
      "metadata": {
        "id": "0ltGWnMTQd6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA FRAME INSEE\n",
        "\n",
        "> Ce Data Frame regroupe les fichiers de l'INSEE recensant les morts de 2000 à 2022.\n",
        "\n",
        "> Ce Data Frame regroupe les fichiers de l'INSEE recensant les morts de 1984 à 1999.\n",
        "\n",
        "> Il suffit de modifier la boucle for ..."
      ],
      "metadata": {
        "id": "jqi24v0WQiMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dossier = '/content/drive/MyDrive/Stage/sas/sas/'\n",
        "# Liste pour stocker les DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Boucle pour créer ma liste de DataFrames\n",
        "\n",
        "for i in tqdm(range(2000,2023)):\n",
        "    # Obtenir la liste des fichiers CSV dans le dossier\n",
        "    chemins_fichiers_sas = dossier+f'a{i}.sas7bdat'\n",
        "    dataframes.append(pd.read_sas(chemins_fichiers_sas))\n",
        "\n",
        "df_ins = pd.concat(dataframes)\n",
        "n = df_ins.shape[0]\n",
        "df_ins['index']= list(range(1,n+1))\n",
        "df_ins.set_index('index', inplace=True)\n",
        "\n",
        "# Data Frame Insee avec les bons noms de colonne et les bons formats\n",
        "df_insee1 = pd.DataFrame(index=list(range(1,df_ins.shape[0])), columns = ['B_nom', 'B_prenoms',\n",
        "        'B_sexe', 'B_annee', 'B_mois', 'B_jour','B_ville', 'B_pays', 'B_departement',\n",
        "        'D_annee', 'D_mois', 'D_jour', 'codebirth', 'nbdeath', 'type', 'reserve', 'fichier', 'allobs',\n",
        "        'nom','prenoms','sexe','dpt_naissance','ybirth','mbirth','dbirth'])\n",
        "\n",
        "\n",
        "df_insee1['nom'] = df_ins['nom'].str.decode('utf-8', 'ignore')[2:-1]\n",
        "df_insee1['B_nom'] = df_ins['nom'].str.decode('utf-8', 'ignore')[2:-1]\n",
        "df_insee1['B_prenoms'] = df_ins['prenoms'].str.decode('utf-8', 'ignore')[2:-1]\n",
        "df_insee1['ybirth'] = df_ins['ybirth']\n",
        "df_insee1['mbirth'] = df_ins['mbirth']\n",
        "df_insee1['dbirth'] = df_ins['dbirth']\n",
        "df_insee1['B_annee'] = df_ins['ybirth']\n",
        "df_insee1['B_mois'] = df_ins['mbirth']\n",
        "df_insee1['B_jour'] = df_ins['dbirth']\n",
        "df_insee1['D_annee'] = df_ins['ydeath']\n",
        "df_insee1['D_mois'] = df_ins['mdeath']\n",
        "df_insee1['D_jour'] = df_ins['ddeath']\n",
        "df_insee1['dpt_naissance'] = (df_ins['codebirth'].str.decode('utf-8')).str.slice(0,2)\n",
        "df_insee1['B_departement'] = (df_ins['codebirth'].str.decode('utf-8')).str.slice(0,2)\n",
        "df_insee1['B_pays'] = df_ins['paysbirth'].str.decode('iso-8859-1')\n",
        "df_insee1['B_ville'] = df_ins['villebirth'].str.decode('iso-8859-1')\n",
        "df_insee1['codebirth'] = df_ins['codebirth']\n",
        "df_insee1['codebirth'] = df_ins['codebirth']\n",
        "df_insee1['nbdeath'] = df_ins['nbdeath']\n",
        "df_insee1['type'] = df_ins['type']\n",
        "df_insee1['reserve'] = df_ins['reserve']\n",
        "df_insee1['fichier'] = df_ins['fichier']\n",
        "df_insee1['allobs'] = df_ins['allobs']\n",
        "df_insee1['B_sexe'] = df_ins['sex']\n",
        "df_insee1['sexe'] = df_ins['sex']\n",
        "df_insee1['B_departement']= pd.to_numeric(df_insee1['B_departement'], errors='coerce').astype(float)\n",
        "df_insee1['dpt_naissance']= pd.to_numeric(df_insee1['B_departement'], errors='coerce').astype(float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP-brhIxQeGG",
        "outputId": "58319b82-e600-4818-a55f-da594e892148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23/23 [03:16<00:00,  8.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA FRAME AGENCE"
      ],
      "metadata": {
        "id": "I4UuHk6cjosJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_grille = pd.read_csv('data_agence.csv', encoding='ISO-8859-1')\n",
        "\n",
        "def convert_to_timestamp(value):\n",
        "    if pd.isnull(value) or value == '/':\n",
        "        return value\n",
        "    else:\n",
        "        try:\n",
        "            timestamp = dateparser.parse(str(value))\n",
        "            return timestamp\n",
        "        except ValueError:\n",
        "            return value\n",
        "\n",
        "df_grille['Date_naissance_vendeur_1'] = df_grille['Date_naissance_vendeur_1'].apply(convert_to_timestamp)\n",
        "df_grille['Date_naissance_vendeur_2'] = df_grille['Date_naissance_vendeur_2'].apply(convert_to_timestamp)\n",
        "df_grille = df_grille.dropna(subset = 'Tete_1')\n",
        "df_grille['Tete_1'] = df_grille['Tete_1'].astype(str)\n",
        "df_grille['Tete_2'] = df_grille['Tete_2'].astype(str)\n",
        "df_grille[df_grille['Tete_1']=='Vendeur 1']['lieutete1'] = df_grille[df_grille['Tete_1']=='Vendeur 1']['Ville_naissance_vendeur_1']\n",
        "df_grille[df_grille['Tete_1']=='Vendeur 2']['lieutete1'] = df_grille[df_grille['Tete_1']=='Vendeur 2']['Ville_naissance_vendeur_2']\n",
        "df_grille[df_grille['Tete_2']=='Vendeur 2']['lieutete2'] = df_grille[df_grille['Tete_2']=='Vendeur 2']['Ville_naissance_vendeur_2']\n",
        "\n",
        "\n",
        "colonnes = ['Adresse_bien_vendu', 'Prenom_vendeur_1', 'Nom_vendeur_1',\n",
        "       'Nom_vendeur_2', 'Nom_acheteur_2', 'Type_de_viager', 'Majoration',\n",
        "       'Bouquet', 'Rente', 'Tete_1', 'Tete_2', 'Commission', 'Milliemes',\n",
        "       'Standing', 'Remarques', 'Valeur_libre', 'Valeur_occupee', 'Numero',\n",
        "       'Repertoire', 'Prenom_vendeur_2', 'Date_naissance_vendeur_1',\n",
        "       'Ville_naissance_vendeur_1', 'Departement_naissance_vendeur_1',\n",
        "       'Pays_naissance_vendeur_1', 'Date_naissance_vendeur_2',\n",
        "       'Ville_naissance_vendeur_2', 'Departement_naissance_vendeur_2',\n",
        "       'Pays_naissance_vendeur_2', 'Adresse_vendeur', 'Nom_acheteur_1',\n",
        "       'Profession_acheteur_1', 'Date_naissance_acheteur_1',\n",
        "       'Profession_acheteur_2', 'Date_naissance_acheteur_2',\n",
        "       'Adresse_acheteur', 'Indexation_rente', 'Date_acte_authentique',\n",
        "       'Payeur_Com', 'Chambre_de_bonnes', 'Nb_pieces', 'Etage',\n",
        "       'Mettres_carres', 'Ascenseur', 'Parking_Box', 'Cave', 'Date_immeuble',\n",
        "       'Pierre_de_taille', 'source', 'Ch_de_service', 'Parking', 'Bonus',\n",
        "       'annee', 'type', 'nom1', 'nom2', 'prenom1', 'prenom2', 'sexe1', 'sexe2',\n",
        "       'annee1', 'annee2', 'naissancetete1', 'mois1', 'jour1',\n",
        "       'naissancetete2', 'mois2', 'jour2', 'lieutete1', 'dep1', 'lieutete2',\n",
        "       'dep2', 'abroad1', 'abroad2', 'quality', 'nom', 'justenom1', 'societe',\n",
        "       'vsociete', 'asociete', 'legasse', 'nblegassenet', 'reprise', 'limite',\n",
        "       'stranger1', 'euro', 'franc', 'downp', 'annuity', 'VL', 'VO', 'done1']\n",
        "\n",
        "colonnes1 = ['nom1', 'prenom1', 'annee1', 'mois1', 'jour1',\n",
        "       'dep1', 'abroad1','lieutete1','sexe1',\n",
        "        'Adresse_bien_vendu', 'Prenom_vendeur_1', 'Nom_vendeur_1',\n",
        "       'Nom_vendeur_2', 'Nom_acheteur_2', 'Type_de_viager', 'Majoration',\n",
        "       'Bouquet', 'Rente', 'Tete_1', 'Tete_2', 'Commission', 'Milliemes',\n",
        "       'Standing', 'Remarques', 'Valeur_libre', 'Valeur_occupee', 'Numero',\n",
        "       'Repertoire', 'Prenom_vendeur_2', 'Date_naissance_vendeur_1',\n",
        "       'Ville_naissance_vendeur_1', 'Departement_naissance_vendeur_1',\n",
        "       'Pays_naissance_vendeur_1', 'Date_naissance_vendeur_2',\n",
        "       'Ville_naissance_vendeur_2', 'Departement_naissance_vendeur_2',\n",
        "       'Pays_naissance_vendeur_2', 'Adresse_vendeur', 'Nom_acheteur_1',\n",
        "       'Profession_acheteur_1', 'Date_naissance_acheteur_1',\n",
        "       'Profession_acheteur_2', 'Date_naissance_acheteur_2',\n",
        "       'Adresse_acheteur', 'Indexation_rente', 'Date_acte_authentique',\n",
        "       'Payeur_Com', 'Chambre_de_bonnes', 'Nb_pieces', 'Etage',\n",
        "       'Mettres_carres', 'Ascenseur', 'Parking_Box', 'Cave', 'Date_immeuble',\n",
        "       'Pierre_de_taille', 'source', 'Ch_de_service', 'Parking', 'Bonus',\n",
        "       'annee', 'type', 'quality', 'justenom1', 'societe',\n",
        "       'vsociete', 'asociete', 'legasse', 'nblegassenet', 'reprise', 'limite',\n",
        "       'stranger1', 'euro', 'franc', 'downp', 'annuity', 'VL', 'VO', 'done1']\n",
        "\n",
        "colonnes2 = ['nom2', 'prenom2', 'annee2', 'mois2', 'jour2',\n",
        "       'dep2', 'abroad2','lieutete2','sexe2',\n",
        "       'Adresse_bien_vendu', 'Prenom_vendeur_1', 'Nom_vendeur_1',\n",
        "       'Nom_vendeur_2', 'Nom_acheteur_2', 'Type_de_viager', 'Majoration',\n",
        "       'Bouquet', 'Rente', 'Tete_1', 'Tete_2', 'Commission', 'Milliemes',\n",
        "       'Standing', 'Remarques', 'Valeur_libre', 'Valeur_occupee', 'Numero',\n",
        "       'Repertoire', 'Prenom_vendeur_2', 'Date_naissance_vendeur_1',\n",
        "       'Ville_naissance_vendeur_1', 'Departement_naissance_vendeur_1',\n",
        "       'Pays_naissance_vendeur_1', 'Date_naissance_vendeur_2',\n",
        "       'Ville_naissance_vendeur_2', 'Departement_naissance_vendeur_2',\n",
        "       'Pays_naissance_vendeur_2', 'Adresse_vendeur', 'Nom_acheteur_1',\n",
        "       'Profession_acheteur_1', 'Date_naissance_acheteur_1',\n",
        "       'Profession_acheteur_2', 'Date_naissance_acheteur_2',\n",
        "       'Adresse_acheteur', 'Indexation_rente', 'Date_acte_authentique',\n",
        "       'Payeur_Com', 'Chambre_de_bonnes', 'Nb_pieces', 'Etage',\n",
        "       'Mettres_carres', 'Ascenseur', 'Parking_Box', 'Cave', 'Date_immeuble',\n",
        "       'Pierre_de_taille', 'source', 'Ch_de_service', 'Parking', 'Bonus',\n",
        "       'annee', 'type', 'quality', 'justenom1', 'societe',\n",
        "       'vsociete', 'asociete', 'legasse', 'nblegassenet', 'reprise', 'limite',\n",
        "       'stranger1', 'euro', 'franc', 'downp', 'annuity', 'VL', 'VO', 'done1']\n",
        "\n",
        "df_travail = pd.DataFrame(index = list(range(1,df_grille['nom1'].count()+df_grille['nom2'].count()+1)),\n",
        "                          columns = colonnes)\n",
        "\n",
        "df1 = df_grille[pd.notnull(df_grille['nom1'])][colonnes1]\n",
        "df2 = df_grille[pd.notnull(df_grille['nom2'])][colonnes2]\n",
        "\n",
        "df1 = df1.rename(columns={'nom1': 'A_nom', 'prenom1' : 'A_prenoms',\n",
        "                          'annee1' : 'A_annee' , 'mois1' : 'A_mois',\n",
        "       'jour1' : 'A_jour', 'lieutete1' : 'A_ville',\n",
        "                          'dep1' : 'A_departement',\n",
        "       'abroad1':'A_abroad', 'sexe1' : 'A_sexe'})\n",
        "df2 = df2.rename(columns={'nom2': 'A_nom', 'prenom2' : 'A_prenoms',\n",
        "                          'annee2' : 'A_annee' , 'mois2' : 'A_mois',\n",
        "       'jour2' : 'A_jour', 'lieutete2' : 'A_ville',\n",
        "                          'dep2' : 'A_departement',\n",
        "       'abroad2':'A_abroad', 'sexe2' : 'A_sexe'})\n",
        "\n",
        "\n",
        "df_travail = pd.concat([df1, df2], ignore_index = True)\n",
        "df_travail['A-departement'] = pd.to_numeric(df_travail['Dpt_naissance'], errors='coerce').astype('Int64').astype(str)\n",
        "df_travail['A_sexe'] = df_travail['sexe'].astype(float)\n",
        "df_travail['A_nom'] = df_travail['nom'].str.upper()\n",
        "\n",
        "# création des colonnes de match avec le df INSEE\n",
        "df_travail['nom'] = df_travail['A_nom']\n",
        "df_travail['sexe'] = df_travail['A_sexe']\n",
        "df_travail['ybirth'] = df_travail['A_annee']\n",
        "df_travail['mbirth'] = df_travail['A_mois']\n",
        "df_travail['dbirth'] = df_travail['A_jour']\n",
        "df_travail['dpt_naissance'] = df_travail['A_departement']\n",
        "\n",
        "# colonne pour pouvoir identifier les individus (et non plus les paires de têtes)\n",
        "df_travail['identification'] = list(range(1,df_travail.shape[0]+1))\n",
        "df_travail = df_travail.append(df_grille[])"
      ],
      "metadata": {
        "id": "_G4TKnkyjr_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MERGE"
      ],
      "metadata": {
        "id": "0wgaSu-tmT81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_travail = pd.read_csv('df_travail.csv')\n",
        "merged_df = df_travail.merge(df_insee1, on=['nom','sexe','ybirth','mbirth','dbirth','dpt_naissance'], how='left')\n",
        "merged_df = merged_df.dropna(subset = ['ydeath'] )\n",
        "\n",
        "# Vérification à l'aide de la liste des prénoms :\n",
        "merged_df['prenoms_x'] = merged_df['prenoms_x'].str.upper()\n",
        "merged_df['prenoms_y'] = merged_df['prenoms_y'].str.upper()\n",
        "liste_verif = []\n",
        "for i in tqdm(merged_df.index.tolist()):\n",
        "\n",
        "  liste_unidecode = []\n",
        "  liste_prenoms = []\n",
        "\n",
        "  for element in str(merged_df['prenoms_y'][i]).split(' '):\n",
        "    liste_unidecode.append(unidecode(element).strip())\n",
        "    liste_prenoms.append(element.strip())\n",
        "\n",
        "  if len(str(merged_df['prenoms_x'][i]).split(' '))==1 and pd.notnull(merged_df['prenoms_x'][i]) :\n",
        "\n",
        "    if merged_df['prenoms_x'][i].strip() in liste_prenoms:\n",
        "      liste_verif.append(merged_df['identification'][i])\n",
        "    elif unidecode(merged_df['prenoms_x'][i]) in liste_unidecode :\n",
        "      liste_verif.append(merged_df['identification'][i])\n",
        "    elif pd.notnull(merged_df['prenoms_y'][i]) and unidecode(merged_df['prenoms_x'][i]).strip() == unidecode(merged_df['prenoms_y'][i]):\n",
        "      liste_verif.append(merged_df['identification'][i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  elif pd.notnull(merged_df['prenoms_x'][i]) and len(merged_df['prenoms_x'][i].split(' '))>1:\n",
        "    for element in merged_df['prenoms_x'][i].split(' '):\n",
        "      if element in liste_prenoms or unidecode(element) in liste_unidecode:\n",
        "        liste_verif.append(merged_df['identification'][i])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "liste_verif = list(set(liste_verif))\n",
        "print(\"Il y a\",len(liste_verif),\"individus pour qui les prénoms coïncident\")"
      ],
      "metadata": {
        "id": "U2-L3QHDmU8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-FJIPKTmm-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBK9YPODmnGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "urEYKUATmnOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hS1r0uE2mizA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}